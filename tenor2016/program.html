<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>TENOR 2016 - Programme</title>

    <!-- Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
  <script>
	function showhide(elt) {
		if (!elt) return;
		if (elt.style.display == 'none')
			elt.style.display = 'inline';
		else
			elt.style.display = 'none';
	}
	function hide(elt) {
		if (elt && elt.style.display == 'inline')
			elt.style.display = 'none';
	}
	function showhideabstract(id) {
		a = document.getElementById('A'+id);
		b = document.getElementById('B'+id);
		hide (b);
		showhide(a);
  	}
	function showhidebibtex(id) {
		a = document.getElementById('A'+id);
		b = document.getElementById('B'+id);
		hide (a);
		showhide(b);
  	}
  </script>

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#page-top">TENOR 2016 Programme</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li class="page-scroll tenormenu">
                        <a href="workshops.html">Workshops</a>
                    </li>
                    <li class="page-scroll tenormenu">
                        <a href="#day1">Day 1</a>
                    </li>
                    <li class="page-scroll tenormenu">
                        <a href="#day2">Day 2</a>
                    </li>
                    <li class="page-scroll tenormenu">
                        <a href="#posters">Posters</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <a href=index.html><img class="img-responsive" src="img/tenor-2016-logo.png" alt="Tenor 2016" width=200 ></a>
                    <div class="intro-text">
                        <hr class="star-light">
                        <span class="wstitle">Programme</span><br /><br />
						<!--h4 style="color: #BB0000; text-shadow: 2px 2px 2px #DCDCDC;">Deadline for camera ready papers is April 4</h4-->
                        <!--img class="img-responsive" src="img/rrsmith.png" width=200-->
                        <span class="where">28 - 29 May 2016<br />
						Anglia Ruskin University, Cambridge, UK</span><br />
                    </div><br />
					<div style="color: #BB0000; text-shadow: 2px 2px 2px #DCDCDC; font-size: 125%">Download all the
					<a href="https://www.tenor-conference.org/proceedings/TENOR2016-Proceedings.pdf">proceedings</a>
					and all the <a href="https://www.tenor-conference.org/proceedings/bibtex2016.bib">bibtex</a> refs.</div>
                </div>
            </div>
        </div>
    </header>


	<!-- day1 -->
	<section id='day1'>
	<div class='container'>
	<div class='row'>
		<div class='col-lg-12 text-center'>
		<h3>Saturday 28th May</h3>
			<hr class='star-primary'>
		</div>
 	<div id='program'>
<div id='location'>Location: LAB 028</div>
	<div id='progintro'>
 	   <p>
 	   <span class='time'>09:00</span> &nbsp;&nbsp;
 	   <span class='registration'> Registration</span>
 	   </p>
<div id='location'>Location: LAB 026</div>
 	   <p>
 	   <span class='time'>09:20</span> &nbsp;&nbsp;
 	   <span class='registration'> Introduction</span>
 	   </p>
	</div>
		<div id='sessionTitle'><a name='1 - Visualisation'>1 - Visualisation</a>
			<span class='right chair'>Chair: Dominique Fober</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>09:30</span> &nbsp;&nbsp;
    	<span class='paper'>Real-Time Corpus-Based Concatenative Synthesis for Symbolic Notation</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("1")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("1")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/01_Ghisi_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Daniele Ghisi and Carlos Agon</span>
    </p>
<div id='A1', style='display: none;'><p class='abstract'>
We introduce a collection of modules designed to segment, analyze, display and sequence symbolic scores in real-time. This mechanism, inspired from CataRT’s corpus-based concatenative synthesis, is implemented as a part of the dada library for Max, currently under development.
</p>
</div>
<div id='B1', style='display: none;'><p class='bibtex'>
@inproceedings{Ghisi_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Daniele Ghisi and Carlos Agon }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Real-Time Corpus-Based Concatenative Synthesis for Symbolic Notation}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {1--7}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>09:50</span> &nbsp;&nbsp;
    	<span class='paper'>Tension ribbons: Quantifying and visualising tonal tension</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("29")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("29")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/02_Herremans_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Dorien Herremans and Elaine Chew</span>
    </p>
<div id='A29', style='display: none;'><p class='abstract'>
Tension is a complex multidimensional concept that is not easily quantified. This research proposes three methods for quantifying aspects of tonal tension based on the spiral array, a model for tonality. The cloud diameter measures the dispersion of clusters of notes in tonal space; the cloud momentum measures the movement of pitch sets in the spiral array; finally, tensile strain measures the distance between the local and global tonal context. The three methods are implemented in a system that displays the results as tension ribbons over the music score to allow for ease of interpretation. All three methods are extensively tested on data ranging from small snippets to phrases with the Tristan chord and larger sections from Beethoven and Schubert piano sonatas. They are further compared to results from an existing empirical experiment.
</p>
</div>
<div id='B29', style='display: none;'><p class='bibtex'>
@inproceedings{Herremans_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Dorien Herremans and Elaine Chew }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Tension ribbons: Quantifying and visualising tonal tension}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {8--18}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>10:10</span> &nbsp;&nbsp;
    	<span class='paper'>Hybrid Real/Mimetic Sound Works</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("35")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("35")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/03_Vickery_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Lindsay Vickery</span>
    </p>
<div id='A35', style='display: none;'><p class='abstract'>
In 2013 I began a project to construct a process allowing for data interchange between visual and sonic media: to create a continuum in which sound could be visualized and then resonified through by both live performers and digital means. A number of processes to aid this visualisation/sonification “ecosystem” were developed. Software was created to create scores based on sonic features of “field recordings” through spectral analysis by rendering the frequency of the strongest detected sinusoidal peak of a recording vertically and its timbral characteristics by luminance, hue and saturation on a scrolling score. Along similar principals a second process was developed to generate a realtime score using graphical symbols to represent detected accents in “found sound” speech recordings. In the other direction software was built to render greyscale images (including sonograms) as sound and a second iteration to generate audio from detected analysis parameters. The imperfections in the various transcription processes are intriguing in themselves as they throw into relief the distinctions between the various forms of representation and in particular the timescales in which they are perceived. The implied circularity of processes also opened the potential for re-interrogation of materials through repeated transmutation. This discussion explores these implications in the context of the analysis of field record-ings to generate visual representations that can be reson-ified using both performative (via notation) and machine (visual data-based) processes, to create hybrid re-al/mimetic sound works through the combination (and recombination) of the processes.
</p>
</div>
<div id='B35', style='display: none;'><p class='bibtex'>
@inproceedings{Vickery_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Lindsay Vickery }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Hybrid Real/Mimetic Sound Works}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {19--24}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>10:30</span> &nbsp;&nbsp;
    	<span class='paper'>Visualizing Music in its Entirety using Acoustic Features: Music Flowgram</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("42")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("42")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/04_Jeong_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Dasaem Jeong and Juhan Nam</span>
    </p>
<div id='A42', style='display: none;'><p class='abstract'>
In this paper, we present an automatic method for visualizing a music audio file from its beginning to end, especially for classical music. Our goal is developing an easy-to-use visualization method that is helpful for listeners and can be used for various kinds of classical music, even for complex orchestral music. To represent musical characteristic, the method uses audio features like volume, onset density, and auditory roughness, which describe loudness, tempo, and dissonance, respectively. These features are visually mapped into static two-dimensional graph, so that users can see how the music changes by time at a look. We have implemented the method with Web Audio API, and it works on recent version of web browsers like Chrome, Firefox, Safari, and Opera. Users can access to the visualization system on their web browser and make visualizations from their own music audio files. Two types of user tests were conducted to verify effects and usefulness of the visualization for classical music listeners. The result shows that it helps listeners to memorize and understand a structure of music, and to easily find a specific part of the music.
</p>
</div>
<div id='B42', style='display: none;'><p class='bibtex'>
@inproceedings{Jeong_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Dasaem Jeong and Juhan Nam }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Visualizing Music in its Entirety using Acoustic Features: Music Flowgram}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {25--32}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
<div id='break'><span class='time'>10:50</span> &nbsp;&nbsp;Break</div>

		<div id='sessionTitle'><a name='2 - Ethnomusicology'>2 - Ethnomusicology</a>
			<span class='right chair'>Chair: Torsten Anders</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>11:10</span> &nbsp;&nbsp;
    	<span class='paper'>Swaralipi: A Framework for Transcribing and Rendering Indic Music Sheet</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("15")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("15")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/05_Misra_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Chandan Misra, Tuhin Chakraborty, Anupam Basu and Baidurya Bhattacharya</span>
    </p>
<div id='A15', style='display: none;'><p class='abstract'>
Creating music in computer system through its music notations requires two primary components. The first one is the mechanisms to encode music notations of respective music genres and the other one is a framework to provide the look and feel of the music written like a published or handwritten music sheet. Popular music scorewriters like Finale, Sibelius, MuseScore can edit, render and playback music transcribed in Staff notation. Being vastly different from the Indic music system in grammar, notation symbols, tonic system and encoding style, the architecture used in the music software for western music cannot cater to the Indic music system. For this reason there is a dearth of such scorewriters for Indic music system which is rich with a variety of musical genres, each different from the others in their unique notation system and language for depicting their lyric. In this paper, we propose a new framework for transcribing and rendering Indic music sheets for different genres of Indic music in computer. This framework is designed to support all major Indic notation systems and Indic language scripts and is explained using three major notation systems and language scripts throughout the paper as a case study.
</p>
</div>
<div id='B15', style='display: none;'><p class='bibtex'>
@inproceedings{Misra_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Chandan Misra and Tuhin Chakraborty and Anupam Basu and Baidurya Bhattacharya }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Swaralipi: A Framework for Transcribing and Rendering Indic Music Sheet}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {33--43}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>11:30</span> &nbsp;&nbsp;
    	<span class='paper'>Notating the Non-Notateable: Digital Notation of Txalaparta Practice</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("36")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("36")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/06_Hurtado_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Enrike Hurtado and Thor Magnusson</span>
    </p>
<div id='A36', style='display: none;'><p class='abstract'>
This paper explores notation practices related to the ancient Basque musical tradition of the txalaparta. It will firstly present the txalaparta practice, introduce the im-provisational rules of txalaparta playing, and discuss attempts in creating notation systems for the instrument. Due to the nature of txalaparta playing, Common West-ern Notation is not a suitable notation, and we will pre-sent the notation system we have developed as part of the Digital Txalaparta project. This system captures the key parts of playing and serves for both playback and a rich documentation of what players actually perform.
</p>
</div>
<div id='B36', style='display: none;'><p class='bibtex'>
@inproceedings{Hurtado_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Enrike Hurtado and Thor Magnusson }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Notating the Non-Notateable: Digital Notation of Txalaparta Practice}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {44--49}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
    <p>
    <span class='time'>11:50</span> &nbsp;&nbsp;
    <span class='kn'>Keynote</span>
    </p>
<div class='row keynote'>
<h4>Elaine Chew</h4>
<h5>Making concrete the ineffable: from music to mathematical models</h5>
<img class="img-keynote"  src="img/elainechew.png" alt="E. Chew" width="80">
Music as conceptualised in the mind and communicated in performance is more than that which is captured in conventional notation. It may be more than that which can be captured in notation, at least in human-readable form. Just how much of music can we make concrete through notation, or represent in graphs or mathematical models? And, of the multitude of possibilities, how do we select for the most relevant and crucial things to represent? Suppose that, in addition, beyond representation we wish to reveal why—why did the performer or composer choose this over that? why is this passage surprising?—thus veering towards questions of music cognition. How will the objective influence that which we devise to notate? We shall examine some of these issues through a series of experiments that aim to make tangible the ineffable nature of practicing and performing music. In particular, we consider the intentions that shape performances and the deeper music structures that guide them. The decisions and entities that musicians grapple with then provide impetus for the music representations.
</div>
<br />
<br />
<div id='location'>Location: LAB 028</div>
    <br /><p>
    <span class='time'>12:30</span> &nbsp;&nbsp;
    <span class='poster'>Poster session and lunch</span>
    </p>
<div id='location'>Location: Recital Hall (HEL 029)</div>
		<div id='sessionTitle'><a name='Music I'>Music I</a>
			<span class='right chair'>Chair: Paul Rhys</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>13:30</span> &nbsp;&nbsp;
    	<span class='paper'>S-notation: A complete musical notation system for scratching and sample music derived from "Theory of Motions"</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("38")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("38")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/07_Sonnenfeld_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Alexander Sonnenfeld and Kjetil Falkenberg Hansen</span>
    </p>
<div id='A38', style='display: none;'><p class='abstract'>
In this paper, we present and discuss S-notation for sample-based music, and particularly for DJ scratching and turntablism. Sonnenfeld developed S-notation based on his Theory of Motion where scratch music is seen as constructions of concurrent musical gestures (motion parameters), and not only turntable actions. The detailed symbolic notation was inspired by traditional musical notation, and among its advantages it covers current musical needs, it can be read and played live in performance, it provides a tool for composers to convey musical ideas, it can be expanded towards new styles and techniques, and it is generalizable to other types of sample-based music. In addition to motion parameters, the new notation system involves an analysis of the sampled sound. Finally, S-notation is also applicable for documenting and for teaching situations.
</p>
</div>
<div id='B38', style='display: none;'><p class='bibtex'>
@inproceedings{Sonnenfeld_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Alexander Sonnenfeld and Kjetil Falkenberg Hansen }, <br />
&nbsp;&nbsp;&nbsp;   Title = {S-notation: A complete musical notation system for scratching and sample music derived from "Theory of Motions"}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {50--57}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>14:15</span> &nbsp;&nbsp;
    	<span class='paper'>Pitchcircle3D: A Case Study in Live Notation for Interactive Music Performance</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("49")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("49")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/08_Hall_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Tom Hall</span>
    </p>
<div id='A49', style='display: none;'><p class='abstract'>
Recent decades have seen the establishment of computer software live notations intended as music scores, affording new modes of interaction between composers, improvisers, performers and audience. This paper presents a live notations project situated within the research domains of algorithmic music composition, improvisation, performance and software interaction design. The software enables the presentation of live animated scores which display 2D and 3D pitch-space representations of note collections including a spiral helix and pitch-class clock. The software has been specifically engineered within an existing sound synthesis environment, SuperCollider, to produce tight integration between sound synthesis and live notation. In a performance context, the live notation is usually presented as both music score and visualisation to the performers and audience respectively. The case study considers the performances of two of the author's contrasting compositions utilising the software. The results thus far from the project demonstrate the ways in which the software can afford different models of algorithmic and improvised interaction between the composer, performers and the music itself. Also included is a summary of feedback from musicians who have used the software in public music performances over a number of years.
</p>
</div>
<div id='B49', style='display: none;'><p class='bibtex'>
@inproceedings{Hall_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Tom Hall }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Pitchcircle3D: A Case Study in Live Notation for Interactive Music Performance}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {58--64}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
<div id='break'><span class='time'>15:00</span> &nbsp;&nbsp;Break</div>

		<div id='sessionTitle'><a name='3 - Musicology'>3 - Musicology</a>
			<span class='right chair'>Chair: Paul Jackson</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>15:20</span> &nbsp;&nbsp;
    	<span class='paper'>Performance Practice of Real-Time Notation</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("4")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("4")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/09_Shafer_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Seth Shafer</span>
    </p>
<div id='A4', style='display: none;'><p class='abstract'>
The primary distinction between real-time and non-real-time notation is the ability for the performer to know ahead of time the exact details of what they will be asked to perform. This paper address the myriad of performance practice issues encountered when the notation of a work loosens its bounds in the world of the fixed and knowable, and instead explores the realms of chance, spontaneity, and interactivity. Some of these issues include: the problem of rehearsal, the problem of ensemble synchronization, the extreme limits of sight reading, failure as a compositional device, new freedoms for the performer and composer, and the new opportunities offered by the ephemerality and multiplicity of real-time notation.
</p>
</div>
<div id='B4', style='display: none;'><p class='bibtex'>
@inproceedings{Shafer_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Seth Shafer }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Performance Practice of Real-Time Notation}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {65--70}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>15:40</span> &nbsp;&nbsp;
    	<span class='paper'>Representing atypical music notation practices: An example with late 17th century music</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("40")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("40")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/10_Zitellini_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Rodolfo Zitellini and Laurent Pugin</span>
    </p>
<div id='A40', style='display: none;'><p class='abstract'>
From the 17th century towards the first decades of the 18th century music notation slowly looses all influences from mensural music, becoming virtually identical to what we would consider common modern notation. But in these five decades of transformation composers did not just suddenly abandon older notation styles, but they were used alongside the ones that would become standard. Void notation, black notation and uncommon tempi were all mixed together. The scholar preparing modern editions of this music is normally forced to normalize all these atypical notations as many software applications do not support them natively. This paper exemplifies the flexibility of the encoding scheme proposed by the Music Encoding Initiative (MEI) and of Verovio, a visualisation library designed for it. The modular approach of these tools means that particular notation systems can be easily added whilst maintaining compatibility to other encoded notations.
</p>
</div>
<div id='B40', style='display: none;'><p class='bibtex'>
@inproceedings{Zitellini_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Rodolfo Zitellini and Laurent Pugin }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Representing atypical music notation practices: An example with late 17th century music}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {71--77}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>16:00</span> &nbsp;&nbsp;
    	<span class='paper'>The Expressive Function in Wor Songs</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("13")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("13")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/11_Palma_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Helena Palma</span>
    </p>
<div id='A13', style='display: none;'><p class='abstract'>
We study some musical and expressive features of traditional Wor vocal music, an ancestral gender of the Biaks (West Papua). A core aspect in Wor songs is the expression of wonder, which Biaks have developed into an Aesthetics of Surprise. We describe some key structural features in the pitch and time domain used as means to express such an aesthetics. We represent the acoustic and prosodic features encoding expressive content by means of an Expressive Function which contains expressive indices with internal structure. We propose an augmented expressive score for the transcription of unaccompanied Wor songs.
</p>
</div>
<div id='B13', style='display: none;'><p class='bibtex'>
@inproceedings{Palma_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Helena Palma }, <br />
&nbsp;&nbsp;&nbsp;   Title = {The Expressive Function in Wor Songs}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {78--84}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
<div id='break'><span class='time'>16:20</span> &nbsp;&nbsp;Break</div>

		<div id='sessionTitle'><a name='4 - Ontology'>4 - Ontology</a>
			<span class='right chair'>Chair: Nicholas Brown</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>16:40</span> &nbsp;&nbsp;
    	<span class='paper'>Is There a Data Model in Music Notation?</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("6")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("6")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/12_Fournier_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Raphaël Fournier-Sn'Iehotta, Philippe Rigaux and Nicolas Travers</span>
    </p>
<div id='A6', style='display: none;'><p class='abstract'>
Scores are structured objects, and we can therefore envisage operations that change the structure of a score, combine several scores, and produce new score instances from some pre-existing material. Current score encodings, however, are designed for rendering and exchange purposes, and cannot directly be exploited as instances of a clear data model supporting algebraic manipulations. We propose an approach that leverages a music content model hidden in score notation, and define a set of composable operations to derive new "scores" from a corpus of existing ones. We show that this approach supplies a high-level tool to express common, useful applications, can easily be implemented on top of standard components, and finally gives rise to interesting conceptual issues related to the modeling of music notation.
</p>
</div>
<div id='B6', style='display: none;'><p class='bibtex'>
@inproceedings{Fournier_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Raphaël Fournier-Sn'Iehotta and Philippe Rigaux and Nicolas Travers }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Is There a Data Model in Music Notation?}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {85--91}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>17:00</span> &nbsp;&nbsp;
    	<span class='paper'>The Ontology of Live Notations Through Assemblage Theory</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("14")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("14")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/13_Kim-Boyle_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>David Kim-Boyle</span>
    </p>
<div id='A14', style='display: none;'><p class='abstract'>
This paper uses assemblage theory to help develop an ontological framework for better understanding live notation practice. Originally developed by Deleuze and Guattari across a range of theoretical writings, assemblage theory is more fully explicated in the work of Manuel de Landa in the more focused context of social ontology. This paper examines the basic concepts of assemblage theory such as material components, expressive capacities, and relations of exteriority and how they may pro-vide useful insights in the analysis of music which explores the creative potential of live notation. The temporal dynamics of non-linear musical forms are discussed and assemblage theory is shown to be a powerful tool for promoting a better understanding of how the various interactions between material and expressive components help catalyze the emergent properties of the assemblage and through it, the ontological identity of a live notation aesthetic practice.
</p>
</div>
<div id='B14', style='display: none;'><p class='bibtex'>
@inproceedings{Kim-Boyle_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { David Kim-Boyle }, <br />
&nbsp;&nbsp;&nbsp;   Title = {The Ontology of Live Notations Through Assemblage Theory}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {92--97}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>17:20</span> &nbsp;&nbsp;
    	<span class='paper'>[Study no. 50][Notational Becoming][Speculations]</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("19")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("19")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/14_Ross-Smith_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Ryan Ross Smith</span>
    </p>
<div id='A19', style='display: none;'><p class='abstract'>
The use of animation in contemporary notational practices has become increasingly prevalent over the last ten years, due in large part to the increased compositional activities throughout Europe, the United Kingdom, and North America, and in particular Iceland and Western Australia.1 The publication of several foundational texts,2 and the materialization of focused scholarly meetings3 and online consolidation projects4 have also contributed to the expansion of this growing field of animated notational practice. The range of compositional ideas repre- sented by these scores is vast, encompassing a wide va- riety of stylistic approaches and technological experimentation. While these ideas often demonstrate intriguing compositional directions, and the unique dynamic functionalities and visual characteristics of animated scores are clearly distinct from traditionally-fixed scores, it is the real-time generative processes of these scores that represent a shift in the very ontology of the musical score. In this paper I speculate on one possible framing for this ontological distinction by focusing on several attributes that, in combination, most explicitly demonstrate this distinction. These include the real-time, process-based qualities of generative animated notations, the openness that enables these procedural functionalities, the displacement of interpretive influence, and the timeliness of these processes in respect to the temporal relationship between generation, representation as notation, and sonic realization. A new work, Study no. 50, will be examined as a practical demonstration of these attributes, and will function as a jumping off point for a speculative discussion of the concept of Notational Becoming.
</p>
</div>
<div id='B19', style='display: none;'><p class='bibtex'>
@inproceedings{Ross-Smith_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Ryan Ross Smith }, <br />
&nbsp;&nbsp;&nbsp;   Title = {[Study no. 50][Notational Becoming][Speculations]}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {98--104}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
<div id='event'>
    <span class='time'>17:40</span> &nbsp;&nbsp;
    <span class='registration'>Reception</span>
</div>
<div id='location'>Location: Recital Hall (HEL 029)</div>
    <div id='event'>
    <span class='time'>19:00</span> &nbsp;&nbsp;
    <span class='registration'>Concert</span>
<div class='concert'>

<a href="https://youtu.be/GCtgHoYxggU?list=PLTtuO11uJypZ8GSerI6qofHsoU9-phT8-" target=_blank><img class=right src=img/video.png width=30></a>
<h6>Elaine Chew, piano</h6>
<div class='work'>Practicing Haydn (2013)</div>
<div class='desc'>by Elaine Chew, Peter Child, Lina Viste Grønli</div>
<div class='work'>Stolen Rhythm (2009)</div>
<div class='desc'> by Cheryl Frances-Hoad / Franz Josef Haydn</div>
<div class='work'>MorpheuS Haydn (2016)</div>
<div class='desc'> by Dorien Herremans, Elaine Chew / Franz Josef Haydn</div>
<div class='work'>Sonata in E-flat, Hob XVI:45, finale</div>
<div class='desc'> by Franz Josef Haydn (with MuSA.RT* tonal visualisations)</div>

<a href="https://youtu.be/ihwCpv2Ia6o?list=PLTtuO11uJypZ8GSerI6qofHsoU9-phT8-" target=_blank><img class=right src=img/video.png width=30></a>
<h6>Katharine Norman</h6>
<div class='work'>A walk I do (2015)</div>
<div class='desc'> Quartertone alto flute, live text animation and audio processing</div>
<div class='desc'> Carla Rees (quartertone alto flute)</div>

<a href="https://youtu.be/CV-eN-UhvcQ?list=PLTtuO11uJypZ8GSerI6qofHsoU9-phT8-" target=_blank><img class=right src=img/video.png width=30></a>
<h6>Richard Hoadley</h6>
<div class='work'>Edge Violations (2016)</div>
<div class='desc'> Bass clarinet, computer and projections</div>
<div class='desc'> Ian Mitchell Clarinet(s)</div>

<a href="https://youtu.be/vX4vdb-2gJc?list=PLTtuO11uJypZ8GSerI6qofHsoU9-phT8-" target=_blank><img class=right src=img/video.png width=30></a>
<h6>Jonathan Impett</h6>
<div class='work'>Spoken (2015)</div>
<div class='desc'> Metatrumpet and computers</div>

</div>


    </div>
	</div>
	</div>
	</div>
	</section>
	<!-- day2 -->
	<section class='altern' id='day2'>
	<div class='container'>
	<div class='row'>
		<div class='col-lg-12 text-center'>
		<h3>Sunday 29th May</h3>
			<hr class='star-light'>
		</div>
 	<div id='program'>
<div id='location'>Location: LAB 026</div>
		<div id='sessionTitle'><a name='5 - Interaction '>5 - Interaction </a>
			<span class='right chair'>Chair: Chris Nash</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>09:30</span> &nbsp;&nbsp;
    	<span class='paper'>Autonomy, Control, and Notation in Interactive Music</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("20")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("20")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/15_Fox_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>K. Michael Fox</span>
    </p>
<div id='A20', style='display: none;'><p class='abstract'>
This paper proposes a conceptualization of notation for interactive musical environments. The notational approach describes the relationship between both human and non-human agents, instead of actions to be taken or sounds to be made. Of critical importance in contemporary networked culture is the degree to which technological devices and networks constrain (or control) the actions of their users. The author has developed a conception of interactivity and notational considerations which instead foreground the autonomous potential of participants and the computational systems. The author analyzes three case studies that demonstrate either a direct connection or a broader conceptual link to the described notational approach. The larger implication is a need for notational systems which do not constrain the identity of the users of interactive systems while also acknowledging and representing the agency of the systems themselves.
</p>
</div>
<div id='B20', style='display: none;'><p class='bibtex'>
@inproceedings{Fox_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { K. Michael Fox }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Autonomy, Control, and Notation in Interactive Music}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {105--109}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>09:50</span> &nbsp;&nbsp;
    	<span class='paper'>Musical Instruments as Scores: A Hybrid Approach</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("22")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("22")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/16_Tomas_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Enrique Tomás</span>
    </p>
<div id='A22', style='display: none;'><p class='abstract'>
The development of new approaches to instrumentality during the decade of 1960 contributed to the dual perception of instruments as scores. For many performers, the instrument became the score of what they played. This artistic hybridization carries substantial questions about the nature of our scores and about the relationships among instruments, performers and musical works. This paper contextualizes the historical origins of this instrumental development within Drucker's theory of performative materiality. Then we examine the nature and notational scheme of this type of scores making use of the concept of "inherent score". Finally, through the analysis of two examples ("tangible scores" and "choreographic objects") and the notions of "affordance" and "constraint", a compositional framework for shaping the inherent instrument score is presented.
</p>
</div>
<div id='B22', style='display: none;'><p class='bibtex'>
@inproceedings{Tomas_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Enrique Tomás }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Musical Instruments as Scores: A Hybrid Approach}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {110--120}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>10:10</span> &nbsp;&nbsp;
    	<span class='paper'>Musicking the Body Electric. The "body:suit:score" as a polyvalent score interface for situational scores.</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("24")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("24")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/17_Bhagwati_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Sandeep Bhagwati, Marcello Giordano, Joanna Berzowska, Alex Bachmayr, Julian Stein, Joseph Browne, Felix Del Tredici, Deborah Egloff, John Sullivan, Marcelo Wanderley and Isabelle Cossette</span>
    </p>
<div id='A24', style='display: none;'><p class='abstract'>
Situational scores, in this paper, are defined as scores that deliver time- and context-sensitive score information to musicians at the moment when it becomes relevant. Mnemonic (rule/style-based) scores are the oldest score models of this type. Lately, animated, interactive, locative scores have added new options to situative scoring. The body:suit:score is an interface currently developed in collaboration of four labs at Concordia and McGill Universities in Montréal - an interface that will allow the musical use of all four types of situational score. Musicians are clad in a body-hugging suit with embedded technology - this suit becomes their score interface. Ultimately intended to enable ensembles to move through performance spaces unencumbered by visual scores and their specific locations, the project currently enters its second year of research-creation. The paper discusses the closely intertwined technological, ergonomic, music-psychology based and artistic decisions that have led to a first bodysuit prototype - a vibrotactile suit for a solo musician. It will also discuss the so-far three etude compositions by Sandeep Bhagwati and Julian Klein for this prototype, and their widely divergent conceptual approaches to an artistic use of the body:suit:score interface. Finally, the paper discusses next steps and emergent problems and opportunities, both technological and artistic.
</p>
</div>
<div id='B24', style='display: none;'><p class='bibtex'>
@inproceedings{Bhagwati_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Sandeep Bhagwati and Marcello Giordano and Joanna Berzowska and Alex Bachmayr and Julian Stein and Joseph Browne and Felix Del Tredici and Deborah Egloff and John Sullivan and Marcelo Wanderley and Isabelle Cossette }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Musicking the Body Electric. The "body:suit:score" as a polyvalent score interface for situational scores.}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {121--126}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>10:30</span> &nbsp;&nbsp;
    	<span class='paper'>Processing of symbolic music notation via multimodal performance data: Brian Ferneyhough’s Lemma-Icon-Epigram for solo piano, phase 1</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("28")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("28")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/18_Antoniadis_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Pavlos Antoniadis and Frédéric Bevilacqua</span>
    </p>
<div id='A28', style='display: none;'><p class='abstract'>
In the “Performance Notes” to his formidable solo piano work Lemma-Icon-Epigram, British composer Brian Ferneyhough proposes a top-down learning strategy: Its first phase would consist in an “overview of gestural patterning”, before delving into the notorious rhythmic intricacies of this most complex notation. In the current paper, we propose a methodology for inferring such patterning from multimodal performance data. In particular, we have a) conducted qualitative analysis of the correlations between the performance data -an audio recording, 12-axis acceleration and gyroscope signals captured by inertial sensors, kinect video and MIDI- and the implicit annotation of pitch during a ‘sight-reading’ performance; b) observed and documented the correspondence between patterns in the gestural signals and patterns in the score annotations and c) produced joint tablature-like representations, which inscribe the gestural patterning back into the notation, while reducing the pitch material by 70-80% of the original. In addition, we have incorporated this representation in videos and interactive multimodal tablatures using the INScore. Our work is drawing from recent studies in the fields of gesture modeling and interaction. It is extending the authors’ previous work on an embodied model of navigation of complex notation and on an application for offline and real-time gestural control of complex notation by the name GesTCom. Future prospects include the probabilistic modeling of gesture-to-notation mappings, towards the design of interactive systems which learn along with the performer while cutting through textual complexity.
</p>
</div>
<div id='B28', style='display: none;'><p class='bibtex'>
@inproceedings{Antoniadis_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Pavlos Antoniadis and Frédéric Bevilacqua }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Processing of symbolic music notation via multimodal performance data: Brian Ferneyhough’s Lemma-Icon-Epigram for solo piano, phase 1}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {127--136}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
<div id='altbreak'><span class='time'>10:50</span> &nbsp;&nbsp;Break</div>

		<div id='sessionTitle'><a name='6 - Technologies'>6 - Technologies</a>
			<span class='right chair'>Chair: Elaine Chew</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>11:10</span> &nbsp;&nbsp;
    	<span class='paper'>INScore expressions to compose symbolic scores</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("8")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("8")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/19_Lepetit-Aimon_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Gabriel Lepetit-Aimon, Dominique Fober, Yann Orlarey and Stéphane Letz</span>
    </p>
<div id='A8', style='display: none;'><p class='abstract'>
INScore is an environment for the design of augmented interactive music scores turned to non-conventional use of music notation. The environment allows arbitrary graphic resources to be used and composed for the music representation. It supports symbolic music notation, described using Guido Music Notation or MusicXML formats. The environment has been extended to provided score level composition using a set of operators that consistently take scores as arguments to compute new scores as output. INScore API supports now score expressions both at OSC and at scripting levels. The work is based on a previous research that solved the issues of the notation consistency across scores composition. This paper focuses on the language level and explains the different strategies to evaluate score expressions.
</p>
</div>
<div id='B8', style='display: none;'><p class='bibtex'>
@inproceedings{Lepetit-Aimon_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Gabriel Lepetit-Aimon and Dominique Fober and Yann Orlarey and Stéphane Letz }, <br />
&nbsp;&nbsp;&nbsp;   Title = {INScore expressions to compose symbolic scores}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {137--143}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>11:30</span> &nbsp;&nbsp;
    	<span class='paper'>OMLILY: Filling the notational gap between composition and performance</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("32")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("32")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/20_Haddad_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Karim Haddad and Carlos Agon</span>
    </p>
<div id='A32', style='display: none;'><p class='abstract'>
This paper describes the design, the development, the usage, limitations and prospect of future development of Omlily, an OpenMusic library, for editing scores with Lilypond, using OM musical editors.
</p>
</div>
<div id='B32', style='display: none;'><p class='bibtex'>
@inproceedings{Haddad_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Karim Haddad and Carlos Agon }, <br />
&nbsp;&nbsp;&nbsp;   Title = {OMLILY: Filling the notational gap between composition and performance}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {144--150}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
    <p>
    <span class='time'>11:50</span> &nbsp;&nbsp;
    <span class='kn'>Keynote</span>
    </p>
<div class='row keynote'>
<h4>Jonathan Impett</h4>
<h5>Notation as hybrid technology</h5>
<img class="img-keynote" src="img/jonathanimpett.jpg" alt="J. Impett" width="80">
The imagining, design and construction of musical objects beyond the scale of vernacular form or language is always technical. On a fundamental level it is also technological; the creation of music is an iterative, distributed process of inscription through technologies. Notation – broadly considered – is the common element of these technologies.
In this talk I approach questions of notation in contemporary music in the light of such a view of historical practices; metaphors range from measurement and control through language to format and symbolic context for action. I argue that the common practice notation of modern Western art music has particular properties which are essential to the strength of that tradition, that characterize it as a contribution to human culture, but which also present challenges to its evolution.
Notation can be viewed as the surface trace of an unconstrained model – the graphical centre of a network of technologies affording actions that may be physical or conceptual. It remains liminal; we consider the interaction of material and virtual elements through the very material on which notation makes its marks. This network must be considered in a cultural context which itself is technologically informed: a discourse of informal concepts and operations. Reflection on our own informal discourse is crucial in formulating approaches to notation in our contemporary hybrid musical practices.
</div>
<br />
<br />
<div id='location'>Location: LAB 028</div>
    <br /><p>
    <span class='time'>12:30</span> &nbsp;&nbsp;
    <span class='poster'>Poster session and lunch</span>
    </p>
<div id='location'>Location: Recital Hall (HEL 029)</div>
		<div id='sessionTitle'><a name='Music II'>Music II</a>
			<span class='right chair'>Chair: Richard Hoadley</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>13:30</span> &nbsp;&nbsp;
    	<span class='paper'>Decibel ScorePlayer</span>
    	<span class='right refs'>
    	<a title='Show abstract' onclick='showhideabstract("100")'> Abstract</a> </span> <br />
&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;
    	<span class='authors'>Cat Hope (bass flute) and Lindsay Vickery (bass clarinet)</span>
    </p>
<div id='A100', style='display: none;'><p class='abstract'>
This workshop introduces the Decibel Score Player, an ipad platform for the reading of animated, digital graphic notation. This is an App developed within the Decibel new music ensemble, that enables the reading and publication of digital editions of graphic scores from a wide range of stylistic avenues. The Decibel ScorePlayer enables network-synchronised scrolling of proportional colour music scores on multiple tablet computers. This is designed to facilitate the reading of scores featuring predominantly graphic notation in rehearsal and performance, and can be used in the creation of new works, as well as the interpretation of existing works. The app comes with five scores built in and the potential to add others, and has features such as the ability to change speed of the piece, and easy to use slider to move to different parts of the piece for rehearsal purposes, the option to view full score or individual parts on different iPads, the facility to trigger the score player from MaxMSP, and vice versa as well as embedded sound in the score file. A desktop 'score creator' application enables users to create the right format file to play in the app, presecribing duration, part, any accompanying audio and formatting.
</p>
</div>
    <p>

    	<span class='time'>14:15</span> &nbsp;&nbsp;
    	<span class='paper'>Study no. 10 for Piano and Electronics</span>
    	<span class='right refs'>
    	<a title='Show abstract' onclick='showhideabstract("101")'> Abstract</a> </span> <br />
&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;
    	<span class='authors'>Ryan Ross Smith</span>
    </p>
<div id='A101', style='display: none;'><p class='abstract'>
Study no. 10 explores the synchronization of a live performer with an electronics component in a generative, animated notational context. The electronics component, built with Max/MSP, reads the score simultaneously with the performer, effectively preparing each performed piano event with a combination of processed samples and sine tones, and subjecting each sound to randomly-determined DSP. The score for Study no. 10 is persistent, featuring an endless stream of consistently-inconsistent, randomly-determined notational materials, leading to a structurally arbitrary sonic instantiation while still preserving the compositional identity of the work.
</p>
</div>
    <p>

    	<span class='time'>15:00</span> &nbsp;&nbsp;
    	<span class='paper'>Performance and Presentation of Hyperions</span>
    	<span class='right refs'>
    	<a title='Show abstract' onclick='showhideabstract("102")'> Abstract</a> </span> <br />
&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;
    	<span class='authors'>Paul Turowski and Kevin Davis</span>
    </p>
<div id='A102', style='display: none;'><p class='abstract'>
University of Virginia PhD candidates Paul Turowski and Kevin Davis will present Hyperions—an original work created by Turowski and revised in collaboration with Davis.  Hyperions, for solo cello (or other solo instrument), blurs the lines between video game, improvisation, and composition. The performance score exists as custom software that features a physics-based game environment as well as real-time detection of amplitude, pitch, and activity level as control parameters. Collisions within the game world act as triggers for the playback and processing of sounds recorded during performance. Chance-based factors, including interactions with non-playable characters (NPCs), allow unique visual and sonic patterns to emerge with each performance.
</p>
</div>
  </div>
<div id='altbreak'><span class='time'>15:45</span> &nbsp;&nbsp;Break</div>

<div id='event'>
    <span class='time'>16:05</span> &nbsp;&nbsp;
    <span class='registration'>TENOR 2017 Presentation</span>
</div>
		<div id='sessionTitle'><a name='7 - Technologies'>7 - Technologies</a>
			<span class='right chair'>Chair: Tom Hall</span>
		</div>
		<div class='papers'>
    <p>

    	<span class='time'>16:15</span> &nbsp;&nbsp;
    	<span class='paper'>Netscore: an Image Server/Client Package for Transmitting Notated Music to Browser and Virtual Reality Interfaces</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("33")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("33")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/21_Carey_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Benedict Carey and Georg Hajdu</span>
    </p>
<div id='A33', style='display: none;'><p class='abstract'>
NetScore is an extension of the existing MaxScore pack-age (Hajdu, Didkovsky) which adds new functionality for the rapid transmission and display of music notation on remote devices through standard modern browsers with WebSocket support. This was seen as a necessary development for MaxScore due to the ubiquity of tablets and other mobile devices, among other advantages for the user, and future applications of the software. We chose a server based solution executed in Java using the Jetty library for both portability between different platforms, and scalability. Novel applications facilitated by NetScore include transmitting scores generated in Max/MSP into virtual reality interfaces and more convenient performance/ rehearsal of real-time generated music, whereby devices commonly on hand such as smartphones, tablets and laptops are used as e-scores without requiring the installation of additional software.
</p>
</div>
<div id='B33', style='display: none;'><p class='bibtex'>
@inproceedings{Carey_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Benedict Carey and Georg Hajdu }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Netscore: an Image Server/Client Package for Transmitting Notated Music to Browser and Virtual Reality Interfaces}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {151--156}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>16:35</span> &nbsp;&nbsp;
    	<span class='paper'>FEATUR.UX: exploiting multitrack information for artistic visualization</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("45")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("45")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/22_Olowe_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Ireti Olowe, Mathieu Barthet, Mick Grierson and Nick Bryan-Kinns</span>
    </p>
<div id='A45', style='display: none;'><p class='abstract'>
FEATUR.UX (Feature - ous) is an audio visualization tool, currently in the process of development, which proposes to introduce a new approach to sound visualization using pre-mixed, independent multitracks and audio feature extraction. Sound visualization is usually performed using a final mix, mono or stereo track of audio. Audio feature extraction is commonly used in the field of music information retrieval to create search and recommendation systems for large music databases rather than generating live visualizations. Visualizing multitrack audio circumvents problems related to the source separation of mixed audio signals and presents an opportunity to examine interdependent relationships within and between separate streams of music. This novel approach to sound visualization aims to provide an enhanced accession to the listening experience corresponding to this use case that employs non-tonal, non-notated forms of electronic music. Findings from prior research studies focused on live performance and preliminary quantitative results from a user survey have provided the basis from which to develop a prototype that will be used throughout an iterative design study to examine the impact of using multitrack audio and audio feature extraction on sound visualization practice.
</p>
</div>
<div id='B45', style='display: none;'><p class='bibtex'>
@inproceedings{Olowe_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Ireti Olowe and Mathieu Barthet and Mick Grierson and Nick Bryan-Kinns }, <br />
&nbsp;&nbsp;&nbsp;   Title = {FEATUR.UX: exploiting multitrack information for artistic visualization}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {157--166}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>16:55</span> &nbsp;&nbsp;
    	<span class='paper'>A robust algebraic framework for high-level music programming</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("18")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("18")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/23_Janin_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>David Janin</span>
    </p>
<div id='A18', style='display: none;'><p class='abstract'>
In this paper, we present a new algebraic model for music programming : tiled musical graphs. It is based on the idea that the definition of musical objects~: what they are, and the synchronization of these objects~: when they should be played, are two orthogonal aspects of music programming that should be kept separate although handled in a combined way. This leads to the definition of an algebra of music objects~: tiled music graphs, which can be combined by a single operator : the tiled product, that is neither sequential nor parallel but both. From a mathematical point of view, this algebra is known to be especially robust since it is an inverse monoid. Various operators such as the reset and the coreset projections derive from these algebra and turned out to be fairly useful for music modeling. From a programming point of view, it provide a high level domain specific language (DSL) that is both hierarchical and modular. This language is currently under implementation in the functional programming language Haskell. From an applicative point of view, various music modeling examples are provided to show how notes, chords, melodies, musical meters and various kind of interpretation aspects can easily and robustly be encoded in this formalism.
</p>
</div>
<div id='B18', style='display: none;'><p class='bibtex'>
@inproceedings{Janin_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { David Janin }, <br />
&nbsp;&nbsp;&nbsp;   Title = {A robust algebraic framework for high-level music programming}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {167--175}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'>17:15</span> &nbsp;&nbsp;
    	<span class='paper'>The Possibilities of a Line: Marking the Glissando in Music</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("47")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("47")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/24_Hope_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Cat Hope and Michael Terren</span>
    </p>
<div id='A47', style='display: none;'><p class='abstract'>
Despite the prevalence of the term "line" in musicology to suggest a trajectory or contour of a melody, these do not embody the line in the Euclidean sense of the word, due to the striated, stepwise nature of pitches in the chromatic scale in traditional harmonic music. The glissando, however, embodies this literal and smooth line without fragmentation and has become a way to align music with other disciplines such as architecture, mathematics and physics. In a more figurative sense, the glissando has been used in a mimetic capacity to signify an irrational, metaphysical otherness. From modernist stochasticism to science fiction film scores, the glissando has a dynamic and complex relationship with representation.<br/>Glissandi explore ideas of sonic trace, surface-ness and stasis. The notation of glissandi, in traditional Western and graphical notation as well as spectrographic visualisation, is presented as a line, its horizontal axis being a measure of time, and its vertical axis being a measure of pitch. This “pitch-time space” enables the consideration of the line as a sonic trace—of motion, gesture or time itself. This also permits the conceptualisation of the line as a surface.<br/>Some glissandi also tend to operate in structural stasis, working against the glissando's function as a sonic trace, thus the glissando-as-stasis, especially as related to drone music, is explored. Deriving inspiration from works by composers, Iannis Xeankis, James Tenney and Giacinto Scelsi, compositional attempts to combine the nature of glissando with drone in the author’s own work are de-scribed, providing an examination of examples of the way glissandi and related concepts can be notated formal-ly, rather than decoratively, in musical works.
</p>
</div>
<div id='B47', style='display: none;'><p class='bibtex'>
@inproceedings{Hope_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Cat Hope and Michael Terren }, <br />
&nbsp;&nbsp;&nbsp;   Title = {The Possibilities of a Line: Marking the Glissando in Music}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {176--180}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>
<div id='event'>
    <span class='time'>17:15</span> &nbsp;&nbsp;
    <span class='registration'>Discussion - Closing</span>
</div>
	</div>
	</div>
	</div>
	</section>
	<!-- posters -->
	<section id='posters'>
	<div class='container'>
	<div class='row'>
		<div class='col-lg-12 text-center'>
		<h3>Posters</h3>
			<hr class='star-primary'>
		</div>
 	<div id='program'>
  <div id='posters'>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Resurrecting a Dinosaur - The Adaptation of Clarence Barlow's Legacy Software Autobusk</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("10")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("10")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/25_Hajdu_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Georg Hajdu</span>
    </p>
<div id='A10', style='display: none;'><p class='abstract'>
This paper aims at describing efforts to conserve and further develop the legacy real-time generative music program AUTOBUSK by Clarence Barlow. We present a case study demonstrating that a simple port of 30+ year old code may not suffice to infuse new life into a project that suffered from the abandonment of the hardware it was developed on. In the process of resurrecting this dinosaur, AUTOBUSK was entirely redesigned for the popular music software environments Max and Ableton Live (via Max for Live) and renamed DJster. It comes in several incarnations, the most recent ones being DJster Autobus for Ableton Live, a device for real-time event generation and DJster Autobus Scorepion, a plugin for the MaxScore Editor. These incarnations take advantage of being embedded in current environments running on modern operating systems and have since acquired some new and useful features. As AUTOBUSK/DJster is based on universal musical principles, which Barlow formalized during the 1970’s while working on his generative piano piece Çoǧluotobüsişletmesi, its algorithms are of general applicability for composers and performers working in diverse fields such as microtonality, interactive installations and/or film music. It has therefore inspired me to lay the foundations of a shorthand notation, which we will discuss in the last section.
</p>
</div>
<div id='B10', style='display: none;'><p class='bibtex'>
@inproceedings{Hajdu_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Georg Hajdu }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Resurrecting a Dinosaur - The Adaptation of Clarence Barlow's Legacy Software Autobusk}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {181--186}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Hexaphonic Guitar Transcription and Visualization</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("31")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("31")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/26_Angulo_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Iñigo Angulo, Sergio Giraldo and Rafael Ramirez</span>
    </p>
<div id='A31', style='display: none;'><p class='abstract'>
Music representation has been a widely researched topic through centuries. Transcription of music through the conventional notation system has dominated the field, for the best part of the last centuries. However, this notational system often falls short of communicating the essence of music to the masses, especially to the people with no music training. Advances in signal processing and computer science over the last few decades have bridged this gap to an extent, but conveying the meaning of music remains a challenging research field. Music visualization is one such bridge, which we explore in this paper. This paper presents an approach to visually represent music produced by a guitar. To achieve this, hexaphonic guitar processing is carried out (i.e. processing each of the six strings as an independent monophonic sound source). Once this information is obtained, different approaches for representing it visually are explored. As a final result, a system is proposed to enrich the musical listening experience, by extending the perceived auditory sensations to include visual stimuli.
</p>
</div>
<div id='B31', style='display: none;'><p class='bibtex'>
@inproceedings{Angulo_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Iñigo Angulo and Sergio Giraldo and Rafael Ramirez }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Hexaphonic Guitar Transcription and Visualization}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {187--192}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Designing Dynamic Networked Scores to Enhance the Experience of Ensemble Music Making</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("41")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("41")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/27_Eldridge_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Alice Eldridge, Ed Hughes and Chris Kiefer</span>
    </p>
<div id='A41', style='display: none;'><p class='abstract'>
This paper describes the impetus for, and design and evaluation of, a pilot project examining the potential for digital, dynamic networked scores to enhance the experience of ensemble music making. We present a new networked score presentation system, and describe how it has evolved through a participatory design approach. Feedback has highlighted key issues concerning synchronisation between conductor, performers and notation, and autonomy and adaptation for performers; we discuss these key points and present our future plans for the project.
</p>
</div>
<div id='B41', style='display: none;'><p class='bibtex'>
@inproceedings{Eldridge_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Alice Eldridge and Ed Hughes and Chris Kiefer }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Designing Dynamic Networked Scores to Enhance the Experience of Ensemble Music Making}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {193--199}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Conversion from Standard MIDI Files to Vertical Line Notation Scores and Automatic Decision of Piano Fingering for Beginners</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("26")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("26")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/28_Saito_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Yasuyuki Saito, Eita Nakamura, Riku Sato, Suguru Agata, Yuu Igarashi and Shigeki Sagayama</span>
    </p>
<div id='A26', style='display: none;'><p class='abstract'>
This paper introduces "vertical line notation'' (VLN) of music for piano beginners, a conversion method from standard MIDI files to VLN scores, and an algorithm of automatic decision of piano fingering for it. Currently, staff notation is widely used for various instruments including piano. However, this notation often appears hard to beginners. On the other hand, VLN is intuitive and easy to understand for piano beginners since it graphically indicates the time order of notes as well as fingering. With the VLN score, piano beginners can make smooth progress with correct fingering. VLN scores are expected to help piano beginners make smooth progress with correct fingering. An issue with VLN is that it is currently created by hand with a spreadsheet software. It would be desirable to automatically produce VLN scores from existing digital scores. In this paper, we propose a method of converting standard MIDI files into VLN scores and an algorithm of automatic fingering decision for piano beginners. Some examples of practical and successful use of VLN scores are shown.
</p>
</div>
<div id='B26', style='display: none;'><p class='bibtex'>
@inproceedings{Saito_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Yasuyuki Saito and Eita Nakamura and Riku Sato and Suguru Agata and Yuu Igarashi and Shigeki Sagayama }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Conversion from Standard MIDI Files to Vertical Line Notation Scores and Automatic Decision of Piano Fingering for Beginners}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {200--211}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Taxonomy and Notation of Spatialization</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("44")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("44")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/29_Ellberger_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Emile Ellberger, Germán Toro Pérez, Linda Cavaliero, Johannes Schuett, Basile Zimmermann and Giorgio Zoia</span>
    </p>
<div id='A44', style='display: none;'><p class='abstract'>
The SSMN Spatial Taxonomy and its symbols libraries, which are the corner stone of the Spatialization Symbolic Music Notation (SSMN) project, emanates from research into composers’ attitudes in this domain. It was conceived as the basis for the development of dedicated notation and rendering tools within the SSMN project. The taxonomy is a systematic representation of all relevant features necessary to specify sound spatiality: shape and acoustic quality of the space, structure, position and movement of sound sources. It is based on single descriptors that can be combined in order to define complex spatial configurations. Descriptors can be transformed locally and globally and can be the object of structural and behavioral operations. The SSMN Spatial Taxonomy proposes a corresponding graphic symbolic representation of descriptors, operations and other functional elements facilitating the communication of creative ideas to performers and technical assistants. This paper focuses on the presentation of the taxonomy and the symbols. Additionally it describes the workflow  proposed for using symbols inside a notation software prototype developed within the project. Finally, further aspects concerning the actual and future developments of SSMN are mentioned.
</p>
</div>
<div id='B44', style='display: none;'><p class='bibtex'>
@inproceedings{Ellberger_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Emile Ellberger and Germán Toro Pérez and Linda Cavaliero and Johannes Schuett and Basile Zimmermann and Giorgio Zoia }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Taxonomy and Notation of Spatialization}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {212--219}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Music Analysis Through Visualization</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("21")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("21")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/30_Li_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Jia Li</span>
    </p>
<div id='A21', style='display: none;'><p class='abstract'>
In this paper analytic visualizations are used to selectively highlight salient musical features in four modern compositions, focusing on micro or macro structures: from motivic pitch contour to large-scale form. At a glance these visualizations allow a quick grasp of the structure and assist listeners to make connections between local features and global trends. Textures obscured by musical notation become more apparent when displayed in a graphical format, such as broad registral shifts, polyphonic streaming, as well as interplay between instruments.  Pitch, timbre and voicing are plotted against time to show large-scale patterns that would otherwise be difficult to recognize in a musical score or compare between different works. Music analysis through compositional data visualization not only makes sense to musicians but also to non-musicians, facilitating collaboration and exchange with artists and technicians in other media.
</p>
</div>
<div id='B21', style='display: none;'><p class='bibtex'>
@inproceedings{Li_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Jia Li }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Music Analysis Through Visualization}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {220--225}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Notation as Temporal Instrument</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("37")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("37")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/31_Maestri_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Eric Maestri</span>
    </p>
<div id='A37', style='display: none;'><p class='abstract'>
In this paper the author proposes a descriptive musicological framework built on the notion of notation as temporal instrument in today's context of electronic music. The principal goal is to discuss a research categorization of musical notation that consider the performative character of musical writing in electronic music performance. In the intentions of the author, this framework could resume the multiple enhancement of the temporal dimension of notation implied by the new means of performance in electronic music.
</p>
</div>
<div id='B37', style='display: none;'><p class='bibtex'>
@inproceedings{Maestri_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Eric Maestri }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Notation as Temporal Instrument}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {226--229}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>Visual Confusion in Piano Notation</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("48")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("48")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/32_Wood_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Marion Wood</span>
    </p>
<div id='A48', style='display: none;'><p class='abstract'>
This series of Reaction Time experiments investigates how quickly notes can be read from a screen and immediately executed on a MIDI keyboard. This makes it possible to study pitch reading and motor coordination in considerable detail away from the customary confounds of rhythm reading or pulse entrainment. The first experiment found that reaction times were slower in extreme keys (3#, 4#, 3b, 4b), even for very experienced sightreaders, a large effect of clef in most individuals, and other results suggesting that, in this simple paradigm at least, reading notation presents more of a difficulty to execution than motor coordination. A second experiment found, in addition, an effect of order in which the notes were presented. A clarified form of notation was devised that disambiguates visual confusion across key signatures, and to some extent across clefs. Initial results from an experiment to contrast traditional noteheads with the clearer ones found substantial improvements in both Reaction Time and accuracy for the clarified notation. The possible applications of improved notation to the wider field of piano playing are discussed.
</p>
</div>
<div id='B48', style='display: none;'><p class='bibtex'>
@inproceedings{Wood_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Marion Wood }, <br />
&nbsp;&nbsp;&nbsp;   Title = {Visual Confusion in Piano Notation}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {230--239}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
    <p>

    	<span class='time'></span> &nbsp;&nbsp;
    	<span class='paper'>From Transcription to Signal Representation: Pitch, Rhythm and Performance</span>
    	<span class='right refs'>
    	<a title='Show bibtex' onclick='showhidebibtex("25")'>Bibtex &nbsp; &nbsp; </a>
    	<a title='Show abstract' onclick='showhideabstract("25")'> Abstract</a> </span> <br />
		<span class='pdf'><a href="https://www.tenor-conference.org/proceedings/2016/33_Tahon_tenor2016.pdf"><img src='img/pdficon.gif' width=20></a></span>
    	<span class='authors'>Marie Tahon and Pierre-Eugène Sitchet</span>
    </p>
<div id='A25', style='display: none;'><p class='abstract'>
Musical transcription is a real challenge, moreover in a folk music context. Signal visualization tools could be of interest for such music. The present paper is a comparison of a musical transcription and two signal representations (pitch and rhythm) applied to a song from the Gwoka repertoire. The study aims at finding similar elements and differences on pitch, rhythm and performance features in both the transcription and the signal visualization. Signal visualization is founded on vowel segmentation, and extraction of pitch and duration information. On the one hand transcription gives general characteristics on the music (harmony, tonality and rhythmic structure) and on the other hand, signal visualization gives performance-related characteristics. The main conclusion is that both approaches are of great interest for understanding such a music.
</p>
</div>
<div id='B25', style='display: none;'><p class='bibtex'>
@inproceedings{Tahon_tenor2016, <br />
&nbsp;&nbsp;&nbsp;   Address = {Cambridge, UK}, <br />
&nbsp;&nbsp;&nbsp;   Author = { Marie Tahon and Pierre-Eugène Sitchet }, <br />
&nbsp;&nbsp;&nbsp;   Title = {From Transcription to Signal Representation: Pitch, Rhythm and Performance}, <br />
&nbsp;&nbsp;&nbsp;   Booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation - TENOR2016}, <br />
&nbsp;&nbsp;&nbsp;   Pages = {240--245}, <br />
&nbsp;&nbsp;&nbsp;   Year = {2016}, <br />
&nbsp;&nbsp;&nbsp;   Editor = {Richard Hoadley and Chris Nash and Dominique Fober}, <br />
&nbsp;&nbsp;&nbsp;   Publisher = {Anglia Ruskin University}, <br />
&nbsp;&nbsp;&nbsp;   ISBN = {978-0-9931461-1-4} <br />
}
</div>
  </div>

	</div>
	</div>
	</div>
	</section>

    <!-- Footer -->
    <footer class="text-center">
        <div class="footer-above">
            <div class="container">
                <div class="row">
				<img src="img/Anglia_Ruskin_Logo_webscreen_M.gif" width=180> &nbsp;
				<td><img src="img/bandeau-grame.jpg" width=140></td>  &nbsp;
				<td><img src="img/cu.png" width=180></td>  &nbsp;
				<td><img src="img/cms.png" width=180></td>
                </div>
                <br />
            </div>
        </div>
        <div class="footer-below">
            <div class="container">
                <div class="row">
                	<a href=index.html>TENOR 2016</a>
                </div>
            </div>
        </div>
    </footer>

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-top page-scroll visible-xs visible-sm">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/freelancer.js"></script>

</body>

</html>
